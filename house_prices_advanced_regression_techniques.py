# -*- coding: utf-8 -*-
"""house-prices-advanced-regression-techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBaTQRVHYcAZA0CHjojcv1fA6eqh3s4r
"""

!pip install xgboost lightgbm shap catboost --quiet

!mkdir -p /content/housing_regression/data

from google.colab import files
uploaded = files.upload()

!ls -l /content

!unzip -l "archive.zip" | head
!unzip -l "archive (1).zip" | head
!unzip -l "archive (2).zip" | head
!unzip -l "archive (3).zip" | head

!ls -l /content/housing_regression/data

!ls -l /content/housing_regression/data

!unzip -o "/content/housing_regression/data/bengaluru.zip" -d "/content/housing_regression/data/bengaluru"
!unzip -o "/content/housing_regression/data/king_county.zip" -d "/content/housing_regression/data/king_county"
!unzip -o "/content/housing_regression/data/melbourne.zip" -d "/content/housing_regression/data/melbourne"
!unzip -o "/content/housing_regression/data/california.zip" -d "/content/housing_regression/data/california"
!unzip -o "/content/housing_regression/data/house-prices-advanced-regression-techniques.zip" -d "/content/housing_regression/data/ames"

import pandas as pd

df_ames = pd.read_csv("/content/housing_regression/data/ames/train.csv")
df_beng = pd.read_csv("/content/housing_regression/data/bengaluru/Bengaluru_House_Data.csv")
df_kc   = pd.read_csv("/content/housing_regression/data/king_county/kc_house_data.csv")
df_mel  = pd.read_csv("/content/housing_regression/data/melbourne/melb_data.csv")
df_ca   = pd.read_csv("/content/housing_regression/data/california/housing.csv")

df_ames.head(), df_beng.head(), df_kc.head(), df_mel.head(), df_ca.head()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def explore_dataset(df, target_col, name="Dataset"):
    print(f"\nüìå {name} ‚Äî Summary")
    print("Shape:", df.shape)
    print("\nMissing values (Top 10):")
    print(df.isna().sum().sort_values(ascending=False).head(10))

    # Target Distribution
    plt.figure(figsize=(6,4))
    sns.histplot(df[target_col], kde=True)
    plt.title(f"{name} ‚Äî {target_col} Distribution")
    plt.show()

    # Correlation with target
    num_cols = df.select_dtypes(include=[np.number]).columns
    if target_col in num_cols:
        corr = df[num_cols].corr()[target_col].sort_values(ascending=False)
        print("\nTop positively correlated features:")
        print(corr.head(10))
        print("\nTop negatively correlated features:")
        print(corr.tail(10))

# Mapping dataset to target column name
target_map = {
    "Ames Housing": (df_ames, "SalePrice"),
    "Bengaluru Housing": (df_beng, "price"),
    "King County Housing": (df_kc, "price"),
    "Melbourne Housing": (df_mel, "Price"),
    "California Housing": (df_ca, "median_house_value"),
}

for name, (df, target) in target_map.items():
    explore_dataset(df, target, name)

import numpy as np
import pandas as pd

def clean_dataset(df, target_col):
    df = df.copy()
    # Drop rows where target is missing
    df = df[df[target_col].notna()].copy()

    # Drop obvious ID columns if present
    id_like_cols = ["Id", "id", "PID"]
    for col in id_like_cols:
        if col in df.columns:
            df = df.drop(columns=[col])

    return df

df_ames_clean = clean_dataset(df_ames, "SalePrice")
df_beng_clean = clean_dataset(df_beng, "price")
df_kc_clean   = clean_dataset(df_kc, "price")
df_mel_clean  = clean_dataset(df_mel, "Price")
df_ca_clean   = clean_dataset(df_ca, "median_house_value")

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

def train_test_split_xy(df, target_col, test_size=0.2, random_state=42):
    X = df.drop(columns=[target_col])
    y = df[target_col]
    return train_test_split(X, y, test_size=test_size, random_state=random_state)

def get_column_types(X):
    num_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
    cat_cols = X.select_dtypes(include=["object", "category", "bool"]).columns.tolist()
    return num_cols, cat_cols

def build_preprocessor(X):
    num_cols, cat_cols = get_column_types(X)

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
        ]
    )
    return preprocessor

from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
# from sklearn.svm import SVR  # optional (slower on large data)

models = {
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(alpha=10.0),
    "Lasso": Lasso(alpha=0.001),
    "ElasticNet": ElasticNet(alpha=0.01, l1_ratio=0.5),
    "RandomForest": RandomForestRegressor(
        n_estimators=300, random_state=42, n_jobs=-1
    ),
    "XGBoost": XGBRegressor(
        n_estimators=400,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        objective="reg:squarederror",
        random_state=42,
        n_jobs=-1
    ),
    "LightGBM": LGBMRegressor(
        n_estimators=400,
        learning_rate=0.05,
        random_state=42
    ),
    # "SVR": SVR()  # you can add this later if you want
}

from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np


def evaluate_models_on_dataset(df, target_col, dataset_name):
    print(f"\nüöÄ Evaluating on {dataset_name}")
    X_train, X_test, y_train, y_test = train_test_split_xy(df, target_col)
    preprocessor = build_preprocessor(X_train)

    rows = []

    for name, model in models.items():
        print(f"  ‚Üí Training {name} ...")

        pipe = Pipeline(steps=[
            ("preprocessor", preprocessor),
            ("model", model)
        ])

        cv_scores = cross_val_score(
            pipe, X_train, y_train,
            cv=5,
            scoring="neg_mean_squared_error",
            n_jobs=-1
        )
        # Convert to RMSE
        cv_rmse_mean = np.sqrt(-cv_scores.mean())
        cv_rmse_std = np.sqrt(cv_scores.std())

        pipe.fit(X_train, y_train)
        y_pred = pipe.predict(X_test)

        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        rows.append({
            "dataset": dataset_name,
            "model": name,
            "cv_RMSE_mean": cv_rmse_mean,
            "cv_RMSE_std": cv_rmse_std,
            "test_RMSE": rmse,
            "test_MAE": mae,
            "test_R2": r2
        })

    results_df = pd.DataFrame(rows)
    return results_df.sort_values("test_RMSE")

all_results = []

all_results.append(
    evaluate_models_on_dataset(df_ames_clean, "SalePrice", "Ames Housing")
)
all_results.append(
    evaluate_models_on_dataset(df_beng_clean, "price", "Bengaluru Housing")
)
all_results.append(
    evaluate_models_on_dataset(df_kc_clean, "price", "King County Housing")
)
all_results.append(
    evaluate_models_on_dataset(df_mel_clean, "Price", "Melbourne Housing")
)
all_results.append(
    evaluate_models_on_dataset(df_ca_clean, "median_house_value", "California Housing")
)

results_all_df = pd.concat(all_results, ignore_index=True)
results_all_df

results_all_df.sort_values(["dataset", "test_RMSE"]).groupby("dataset").head(3)

from sklearn.model_selection import RandomizedSearchCV

df = df_ames_clean.copy()
target_col = "SalePrice"

X_train, X_test, y_train, y_test = train_test_split_xy(df, target_col)
preprocessor = build_preprocessor(X_train)

xgb_base = XGBRegressor(
    objective="reg:squarederror",
    random_state=42,
    n_jobs=-1
)

param_dist = {
    "model__n_estimators": [200, 400, 600],
    "model__max_depth": [3, 4, 5, 6, 8],
    "model__learning_rate": [0.01, 0.03, 0.05, 0.1],
    "model__subsample": [0.6, 0.8, 1.0],
    "model__colsample_bytree": [0.6, 0.8, 1.0]
}

pipe_xgb = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", xgb_base)
])

search_xgb = RandomizedSearchCV(
    estimator=pipe_xgb,
    param_distributions=param_dist,
    n_iter=20,
    cv=5,
    scoring="neg_root_mean_squared_error",
    n_jobs=-1,
    random_state=42,
    verbose=1
)

search_xgb.fit(X_train, y_train)

print("Best params:", search_xgb.best_params_)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

y_pred = search_xgb.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)           # ‚úÖ manual RMSE
r2 = r2_score(y_test, y_pred)

print("Test RMSE:", rmse)
print("Test R¬≤:", r2)

import shap

best_pipe = search_xgb.best_estimator_
best_model = best_pipe.named_steps["model"]
best_preprocessor = best_pipe.named_steps["preprocessor"]

# Get preprocessed feature matrix
X_train_raw = X_train.copy()
X_train_transformed = best_preprocessor.transform(X_train_raw)

# SHAP for tree model
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_train_transformed)

# Summary plot
shap.summary_plot(shap_values, X_train_transformed)

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor  # robust & no extra deps
from sklearn.metrics import mean_squared_error, r2_score

# Use the cleaned Ames DF you already have:
# df_ames_clean

deploy_features = [
    "OverallQual",
    "GrLivArea",
    "GarageCars",
    "TotalBsmtSF",
    "FullBath",
    "YearBuilt",
    "LotArea",
    "Neighborhood",
]

target_col = "SalePrice"

df_deploy = df_ames_clean[deploy_features + [target_col]].dropna(subset=[target_col])

X = df_deploy[deploy_features]
y = df_deploy[target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

num_cols = X_train.select_dtypes(include=["int64", "float64"]).columns.tolist()
cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols),
    ]
)

rf_model = RandomForestRegressor(
    n_estimators=400,
    random_state=42,
    n_jobs=-1
)

pipe_ames = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", rf_model)
])

pipe_ames.fit(X_train, y_train)

y_pred = pipe_ames.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("Ames deploy model RMSE:", rmse)
print("Ames deploy model R¬≤:", r2)

neighborhoods = sorted(X["Neighborhood"].dropna().unique().tolist())

!pip install gradio --quiet

import gradio as gr

def predict_price(
    overall_qual, gr_liv_area, garage_cars,
    total_bsmt_sf, full_bath, year_built,
    lot_area, neighborhood
):
    data = {
        "OverallQual": [overall_qual],
        "GrLivArea": [gr_liv_area],
        "GarageCars": [garage_cars],
        "TotalBsmtSF": [total_bsmt_sf],
        "FullBath": [full_bath],
        "YearBuilt": [year_built],
        "LotArea": [lot_area],
        "Neighborhood": [neighborhood],
    }

    df_input = pd.DataFrame(data)
    pred = pipe_ames.predict(df_input)[0]

    return f"Estimated Sale Price: ${pred:,.0f}"


with gr.Blocks() as demo:
    gr.Markdown("# üè† Ames House Price Prediction Dashboard")
    gr.Markdown(
        "This app uses a **Random Forest regression model** trained on the "
        "Ames Housing dataset. The full project includes comparison of multiple "
        "regression models across 5 different housing datasets."
    )

    with gr.Tab("üîÆ Predict Price"):
        gr.Markdown("### Enter property details to get a price estimate")

        with gr.Row():
            overall_qual = gr.Slider(1, 10, value=5, step=1, label="Overall Quality (1‚Äì10)")
            gr_liv_area = gr.Number(value=1500, label="Above Ground Living Area (sq ft)")

        with gr.Row():
            garage_cars = gr.Slider(0, 4, value=2, step=1, label="Garage Capacity (cars)")
            total_bsmt_sf = gr.Number(value=800, label="Total Basement Area (sq ft)")

        with gr.Row():
            full_bath = gr.Slider(0, 4, value=2, step=1, label="Full Bathrooms")
            year_built = gr.Number(value=2000, label="Year Built")

        lot_area = gr.Number(value=8000, label="Lot Area (sq ft)")
        neighborhood = gr.Dropdown(choices=neighborhoods, value=neighborhoods[0], label="Neighborhood")

        predict_btn = gr.Button("Predict Price üí∞")
        output_text = gr.Markdown()

        predict_btn.click(
            fn=predict_price,
            inputs=[
                overall_qual, gr_liv_area, garage_cars,
                total_bsmt_sf, full_bath, year_built,
                lot_area, neighborhood
            ],
            outputs=output_text
        )

    # Second tab: Show your regression results table from earlier
    with gr.Tab("üìä Model Benchmark (5 Datasets)"):
        gr.Markdown(
            "### Regression Model Comparison\n"
            "This table summarizes RMSE, MAE, and R¬≤ for multiple models "
            "across 5 housing datasets (Ames, Bengaluru, King County, Melbourne, California)."
        )

        try:
            # If results_all_df is in memory from earlier steps
            results_df_component = gr.Dataframe(
                value=results_all_df,
                label="Model Performance Summary",
                interactive=False
            )
        except NameError:
            gr.Markdown(
                "‚ö†Ô∏è `results_all_df` not found. "
                "Re-run the cell where you computed all_results / results_all_df."
            )

demo.launch(share=True)